# -*- coding: utf-8 -*-
"""Model Evaluation II

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fayqWmyf-_RamH2WPdxjcBwNNsxVUi15
"""

# preamble to be able to run notebooks in Jupyter and Colab
try:
    from google.colab import drive
    import sys
    
    drive.mount('/content/drive')
    notes_home = "/content/drive/My Drive/csc310/"
    user_home = "/content/drive/My Drive/"
    
    sys.path.insert(1,notes_home) # let the notebook access the notes folder

except ModuleNotFoundError:
    notes_home = "" # running native Jupyter environment -- notes home is the same as the notebook
    user_home = ""  # under Jupyter we assume the user directory is the same as the notebook

import pandas as pd
from sklearn import tree
from treeviz import tree_print
import numpy as np

"""##**Data:**"""

abalone = pd.read_csv(notes_home+"abalone.csv")
abalone

# set up our sklearn data shape for the penguin data
from sklearn.model_selection import train_test_split
abalone2 = pd.read_csv(notes_home+"abalone.csv")
X  = abalone2.drop(['Sex'],axis=1)
y = abalone2['Age']

# split the data - 70% training 30% testing
datasets = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=2)
X_train, X_test, y_train, y_test = datasets

"""##**First test:**"""

from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix


# setting up grid search using 5-fold CV
model = tree.DecisionTreeClassifier(random_state=1)
param_grid = {
    'max_depth': list(range(1,5)),
    'criterion': ['entropy', 'gini']
    }
grid = GridSearchCV(model, param_grid, cv=5)

# performing grid search 
null = grid.fit(X,y)

# print out what we found
print("Best parameters: {}".format(grid.best_params_))

# print out the best model
print("Best tree:")
tree_print(grid.best_estimator_,X)

# compute and print the accuracy
predict_y = grid.best_estimator_.predict(X)
acc = accuracy_score(y, predict_y)
print("Accuracy: {:3.2f}".format(acc))

"""##**Second Test:**"""

abalone3 = pd.read_csv(notes_home+"abalone.csv")
X  = abalone3.drop(['Sex','Age'],axis=1)
y = abalone3['Age']

# split the data - 70% training 30% testing
datasets = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=2)
X_train, X_test, y_train, y_test = datasets
# setting up grid search using 5-fold CV
model = tree.DecisionTreeClassifier(random_state=1)
param_grid = {
    'max_depth': list(range(1,11)),
    'criterion': ['entropy', 'gini']
    }
grid = GridSearchCV(model, param_grid, cv=5)

# performing grid search 
null = grid.fit(X,y)

# print out what we found
print("Best parameters: {}".format(grid.best_params_))

# print out the best model
print("Best tree:")
tree_print(grid.best_estimator_,X)

# compute and print the accuracy
predict_y = grid.best_estimator_.predict(X)
acc = accuracy_score(y, predict_y)
print("Accuracy: {:3.2f}".format(acc))

"""##**Optimal Test Confidence Interval:**"""

# cross-validation Abalone

np.set_printoptions(formatter={'float_kind':"{:3.2f}".format})

# grab cross validation code

# get data
df2 = pd.read_csv(notes_home+"abalone.csv")
X  = df.drop(['Sex'],axis=1)
y = df['Age']

# set up the model
model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)

# do the 5-fold cross validation
scores = cross_val_score(model, X, y, cv=5)
print("Fold Accuracies: {}".format(scores))

# compute 95% confidence intervals for classification and regression
# problems

def classification_confint(acc, n):
    '''
    Compute the 95% confidence interval for a classification problem.
      acc -- classification accuracy
      n   -- number of observations used to compute the accuracy
    Returns a tuple (lb,ub)
    '''
    import math
    interval = 1.96*math.sqrt(acc*(1-acc)/n)
    lb = max(0, acc - interval)
    ub = min(1.0, acc + interval)
    return (lb,ub)

def regression_confint(rs_score, n, k):
    '''
    Compute the 95% confidence interval for a regression problem.
      rs_score -- R^2 score
      n        -- number of observations used to compute the R^2 score
      k        -- number of independent variables in dataset
    Returns a tuple (lb,ub)
    Reference:
    https://books.google.com/books?id=gkalyqTMXNEC&pg=PA88#v=onepage&q&f=false
    '''
    import math
    interval = 2*math.sqrt((4*rs_score*(1-rs_score)**2*(n-k-1)**2)/((n**2 - 1)*(n+3)))
    lb = max(0, rs_score - interval)
    ub = min(1.0, rs_score + interval)
    return (lb,ub)

# read the data
df3 = pd.read_csv(notes_home+"abalone.csv")

# set up the feature matrix and target vector
X  = df3.drop(['Sex',],axis=1)
y = df3['Age']

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=2)

# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.
model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=7, random_state=1)

# fit the model on the training set of data
model.fit(X_train, y_train)

# Test results: evaluate the model on the testing set of data
y_test_model = model.predict(X_test)
acc = accuracy_score(y_test, y_test_model)
observations = X_test.shape[0]
lb,ub = classification_confint(acc, observations)
print("Accuracy: {:3.2f} ({:3.2f},{:3.2f})".format(acc,lb,ub))

"""##**Other Confidence Interval, out of curiosity:**"""

# cross-validation Abalone
np.set_printoptions(formatter={'float_kind':"{:3.2f}".format})

# grab cross validation code

# get data
df = pd.read_csv(notes_home+"abalone.csv")
X  = df.drop(['Sex','Age'],axis=1)
y = df['Age']

# set up the model
model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10)

# do the 5-fold cross validation
scores = cross_val_score(model, X, y, cv=5)
print("Fold Accuracies: {}".format(scores))

# read the data
df4 = pd.read_csv(notes_home+"abalone.csv")

# set up the feature matrix and target vector
X  = df4.drop(['Sex','Age'],axis=1)
y = df4['Age']

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=2)

# set up the tree model object - limit the complexity to put us somewhere in the middle of the graph.
model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=1)

# fit the model on the training set of data
model.fit(X_train, y_train)

# Test results: evaluate the model on the testing set of data
y_test_model = model.predict(X_test)
acc = accuracy_score(y_test, y_test_model)
observations = X_test.shape[0]
lb,ub = classification_confint(acc, observations)
print("Accuracy: {:3.2f} ({:3.2f},{:3.2f})".format(acc,lb,ub))